{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28accc5",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
    "---\n",
    "\n",
    "Teaching Team:\n",
    "\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
    "\n",
    "School of Information Technology, Monash University, Malaysia\n",
    "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
    "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
    "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
    "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
    "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af82432",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Group Information</span>\n",
    "---\n",
    "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
    "\n",
    "Your tutorial day and time: **[Friday 4-6pm]**    <br/>\n",
    "\n",
    "1st group member\n",
    "\n",
    "Surname: **Chok**  <br/>\n",
    "Firstname: **Huey Yan**    <br/>\n",
    "Student ID: **34064451**    <br/>\n",
    "Email: **hcho0085@student.monash.edu**    <br/>\n",
    "\n",
    "2nd group member\n",
    "\n",
    "Surname: **Cheng**  <br/>\n",
    "Firstname: **Yanly**    <br/>\n",
    "Student ID: **34051694**    <br/>\n",
    "Email: **yche0660@student.monash.edu**    <br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027af0",
   "metadata": {},
   "source": [
    "# Streaming Application\n",
    "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb43d2",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
    "\n",
    "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
    "\n",
    "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
    "\n",
    "This assignment contain **3 parts**:\n",
    "\n",
    "* **Part 1**: MongoDB Data Model (5 Marks)\n",
    "* **Part 2**: Streaming Application (20 Marks)\n",
    "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
    "* **Part 4**: Code demo and interview (Negative marking)\n",
    "\n",
    "Required Software:\n",
    "\n",
    "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
    "* Please use the provided Docker to load the Jupyter Notebook\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6830c",
   "metadata": {},
   "source": [
    "### Assignment Marking\n",
    "\n",
    "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652a7b4",
   "metadata": {},
   "source": [
    "### What to Submit\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
    "\n",
    "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
    "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
    "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
    "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
    "\n",
    "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
    "\n",
    "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f836d",
   "metadata": {},
   "source": [
    "### Plagiarism and Collusion\n",
    "\n",
    "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
    "\n",
    "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99713db4",
   "metadata": {},
   "source": [
    "### Generative AI usage\n",
    "\n",
    "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
    " \n",
    "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
    "You may use AI to help you learn how to solve the assignment.\n",
    "\n",
    "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
    " \n",
    "Any work submitted for a mark must:\n",
    "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
    "adhere to the guidelines for AI use set for the assessment task.\n",
    "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
    "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496b475",
   "metadata": {},
   "source": [
    "### Late submissions \n",
    "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384a47",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Preliminary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe6061",
   "metadata": {},
   "source": [
    "### Scenario Background\n",
    "\n",
    "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
    "\n",
    "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
    "\n",
    "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8e2b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 1 - Overview of AWAS</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9965391",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
    "\n",
    "#### vehicle.csv\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* owner_name (a string that contains name of the owner)\n",
    "* owner_addr (a string that contains the address of the owner)\n",
    "* vechicle_type (a string that represents the vechile model)\n",
    "* registration_date (date and time when the vehicle was registered)\n",
    "\n",
    "#### camera.csv\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* latitude (a float value representing latitude of camera)\n",
    "* longitude (a float value representing longitude of camera)\n",
    "* position (a float value tells at which kilometer point is the camera)\n",
    "* speed_limit (a float value of maximum legal speed for the segment)\n",
    "\n",
    "#### camera_event.csv\n",
    "* event_id (a string-based unique identifier to camera reading)\n",
    "* batch_id (a integer-based identifier to batch reading)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
    "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
    "\n",
    "#### camera_event_historic.csv\n",
    "* violation_id (a string-based unique identifier for violation record)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
    "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
    "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
    "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
    "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
    "\n",
    "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a9aab",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pip statement if necessary\n",
    "# pip install jsonschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadfa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any imports here\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import json\n",
    "from pymongo import MongoClient, ASCENDING, HASHED\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, split, element_at, when, from_json, expr, unix_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType, TimestampType, StructField\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n",
    "\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b0dee",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
    "\n",
    "This section consists of 3 sub-questions\n",
    "\n",
    "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97125d",
   "metadata": {},
   "source": [
    "### Task 1.1 Collection Design\n",
    "\n",
    "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
    "* Vehicle (Store static metadata about each vehicle)\n",
    "* Camera (Store static definitions of each camera)\n",
    "* Violation (Records of flagged violations)\n",
    "\n",
    "For each collection, provide\n",
    "* 1-2 sentence description of why this collection exists\n",
    "* document schema and a sample document\n",
    "* indexes (if any) by specifying\n",
    "    * Fields (and sort order if applicable)\n",
    "    * Type\n",
    "    * Purpose of the index\n",
    "* shard key strategy (if any) by specifying\n",
    "    * Chosen shard key\n",
    "    * Shard key type\n",
    "    * Rationale\n",
    "* data retention policy (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db38d5",
   "metadata": {},
   "source": [
    "### Vehicle collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9178758",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vehicle collection\n",
    "# Document schema\n",
    "sample_vehicle_schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"Vehicle\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"_id\", \"owner_name\", \"owner_addr\", \"vehicle_type\", \"registration_date\"],\n",
    "    \"properties\": {\n",
    "        \"_id\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Unique car plate number\"\n",
    "        },\n",
    "        \"owner_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Full name of the vehicle owner\"\n",
    "        },\n",
    "        \"owner_addr\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Postal address of the vehicle owner\"\n",
    "        },\n",
    "        \"vehicle_type\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Type or model of the vehicle\"\n",
    "        },\n",
    "        \"registration_date\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"ISO 8601 timestamp\"\n",
    "        }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# sample JSON document representing a vehicle\n",
    "sample_vehicle = {\n",
    "  \"_id\": \"FT 02\",\n",
    "  \"owner_name\": \"Goh Mei Wei\",\n",
    "  \"owner_addr\": \"943 Jalan Bukit Mawar, Kuala Lumpur\",\n",
    "  \"vehicle_type\": \"Coupe\",\n",
    "  \"registration_date\": \"2006-08-22T03:18:00Z\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887d733",
   "metadata": {},
   "source": [
    "#### Why this collection exists\n",
    "The vehicle collection stores static metadata about each registered vehicle from the csv data. This collection is used for quick ownership and vehicle-type lookup during violation processing.\n",
    "\n",
    "#### Indexes\n",
    "Fields     : \"_id\"  \n",
    "Type       : default primary key   \n",
    "Purpose    : Quick lookup by car_plate.\n",
    "\n",
    "#### Data retention policy\n",
    "Policy: Permanent (no expiration).  \n",
    "Reason: Vehicles remain registered for years and serve as long-term reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93430b6c",
   "metadata": {},
   "source": [
    "### Camera collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Camera collection\n",
    "# Document schema\n",
    "\n",
    "sample_camera_schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"Camera\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"_id\", \"latitude\", \"longitude\", \"position_km\", \"speed_limit\"],\n",
    "    \"properties\": {\n",
    "        \"_id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Unique identifier for the camera\"\n",
    "        },\n",
    "        \"latitude\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": -90,\n",
    "            \"maximum\": 90,\n",
    "            \"description\": \"Latitude of the camera's location\"\n",
    "        },\n",
    "        \"longitude\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": -180,\n",
    "            \"maximum\": 180,\n",
    "            \"description\": \"Longitude of the camera's location\"\n",
    "        },\n",
    "        \"position_km\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"KM marker on the road/highway where the camera is installed\"\n",
    "        },\n",
    "        \"speed_limit\": {\n",
    "            \"type\": \"number\",\n",
    "            \"description\": \"Legal speed limit at this camera segment in km/h\"\n",
    "        }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "# sample JSON document representing a camera\n",
    "sample_camera = {\n",
    "    \"_id\": 1,\n",
    "    \"latitude\": 2.157730731,\n",
    "    \"longitude\": 102.6601002,\n",
    "    \"position_km\": 152.5,\n",
    "    \"speed_limit\": 110\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9974160",
   "metadata": {},
   "source": [
    "#### Why this collection exists\n",
    "Stores static geographic and legal configuration for every speed enforcement camera. Used to determine speed limits, calculate distances between checkpoints, and visualize camera layout.\n",
    "\n",
    "#### Index\n",
    "Fields     : \"_id\"  \n",
    "Type       : default primary key   \n",
    "Purpose    : Fast access to camera configuration.\n",
    "\n",
    "Fields     : \"latitude\", \"longitude\" \n",
    "Type       : composite key   \n",
    "Purpose    : Access to camera configuration using unique coordinates.\n",
    "\n",
    "#### Data retention policy\n",
    "Policy: Permanent (no expiration).  \n",
    "Reason: Camera infrastructure is stable; updates only when infrastructure changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b10c9",
   "metadata": {},
   "source": [
    "### Violation Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ae26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Violation collection\n",
    "# Document schema\n",
    "sample_violation_schema = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"ViolationSummary\",\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"_id\", \"car_plate\", \"date\", \"violations\", \"total_violations\"],\n",
    "    \"properties\": {\n",
    "        \"_id\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Composite key combining car plate and date\"\n",
    "        },\n",
    "        \"car_plate\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"License plate of the vehicle\"\n",
    "        },\n",
    "        \"date\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Date of the violations in YYYY-MM-DD format\"\n",
    "        },\n",
    "        \"violations\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of violation records\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"required\": [\n",
    "                    \"violation_id\", \"type\", \"camera_id_start\", \"camera_id_end\",\n",
    "                    \"timestamp_start\", \"recorded_speed\", \"speed_limit\"\n",
    "                ],\n",
    "                \"properties\": {\n",
    "                    \"violation_id\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Unique identifier for the violation\"\n",
    "                    },\n",
    "                    \"type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"instant\", \"average\"],\n",
    "                        \"description\": \"Violation type\"\n",
    "                    },\n",
    "                    \"camera_id_start\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Starting camera ID\"\n",
    "                    },\n",
    "                    \"camera_id_end\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"Ending camera ID\"\n",
    "                    },\n",
    "                    \"timestamp_start\": {\n",
    "                        \"type\": \"string\",\n",
    "                    },\n",
    "                    \"timestamp_end\": {\n",
    "                        \"type\": [\"string\", \"null\"],\n",
    "                    },\n",
    "                    \"recorded_speed\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Measured speed of the vehicle\"\n",
    "                    },\n",
    "                    \"speed_limit\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"Legal speed limit\"\n",
    "                    }\n",
    "                },\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"total_violations\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 0,\n",
    "            \"description\": \"Total number of violations for the day\"\n",
    "        }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "\n",
    "# sample JSON document representing a violation\n",
    "sample_violation = {\n",
    "    \"_id\": \"2024-01-01_FT 02\",\n",
    "    \"car_plate\": \"FT 02\",\n",
    "    \"date\": \"2024-01-01\",\n",
    "    \"violations\": [\n",
    "        {\n",
    "            \"violation_id\": \"0ff38c74-7ee6-41cd-bc26-3a6060604a02\",\n",
    "            \"type\": \"instant\",\n",
    "            \"camera_id_start\": 1,\n",
    "            \"camera_id_end\": 1,\n",
    "            \"timestamp_start\": \"2024-01-01T08:08:01Z\",\n",
    "            \"timestamp_end\": None,\n",
    "            \"recorded_speed\": 150.3,\n",
    "            \"speed_limit\": 110.0\n",
    "        },\n",
    "        {\n",
    "            \"violation_id\": \"2cf78fb9-bea2-4dd8-951a-0476057e9710\",\n",
    "            \"type\": \"average\",\n",
    "            \"camera_id_start\": 1,\n",
    "            \"camera_id_end\": 2,\n",
    "            \"timestamp_start\": \"2024-01-01T08:08:01Z\",\n",
    "            \"timestamp_end\": \"2024-01-01T08:08:27Z\",\n",
    "            \"recorded_speed\": 133.5,\n",
    "            \"speed_limit\": 110.0\n",
    "        }\n",
    "    ],\n",
    "    \"total_violations\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe87733",
   "metadata": {},
   "source": [
    "#### Why this collection exists\n",
    "Stores confirmed average-speed detected by comparing timestamps across camera checkpoints, or instant-speed violations. Only one record for a car per day is recorded in the database. If the car violates at different cameras, the record should be merged together into a single record to be stored in the database.\n",
    "\n",
    "#### Index\n",
    "Fields     : \"_id\"  \n",
    "Type       : default primary key   \n",
    "Purpose    : Build by combining date and unique car plate number \"date_carplate\" . Fast access violations made by one car in a specific date.\n",
    "\n",
    "Fields     : \"car_plate\", \"date\"  \n",
    "Type       : composite key   \n",
    "Purpose    : Enables filtering by vehicle and date.\n",
    "\n",
    "Fields     : \"violations.violation_id\"  \n",
    "Type       : Multikey   \n",
    "Purpose    : Supports queries to locate specific violations across summaries.\n",
    "\n",
    "#### Data retention policy\n",
    "Policy: Retain for 5–7 years.   \n",
    "Reason: Required for legal audit, appeals, and trend analysis.   \n",
    "Implementation: Add a violation_flagged_at timestamp and optionally use TTL for automatic purge of >7-year-old documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db5c3",
   "metadata": {},
   "source": [
    "### Task 1.2 Collection Relationship\n",
    "\n",
    "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
    "* Read/write patterns\n",
    "* Data duplication versus Join cost\n",
    "* Consistency requirements (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58284962",
   "metadata": {},
   "source": [
    "Violation --> Vehicle (many to 1)\n",
    "Relationship key: Violation.car_plate --> Vehicle.car_plate \n",
    "\n",
    "Violation --> Camera (many to 1)\n",
    "Relationship key: Violation.camera_id --> Camera._id \n",
    "\n",
    "Violation collection will perform a lot of writing during streaming inserts, while vehicle collection is relatively less updated but is read-heavy. Therefore reference to car_plate for lookups which avoids redundancy is chosen over embedding as it would cause duplication in the record. If we store references, there will be no repeated owner or camera data in violation that can help reduce risk of exceeding document size and updating static metadata is changed once. However, for join cost  In consistency context, vehicle updates are rare and does not affect past violations, references can keep violation records linked to updated vehicle info without document rewrites.\n",
    "\n",
    "Since Camera is static whereas Violation is dynamic, embedding would not be suitable as it would increase the write size and lead to stale data if camera detail is changed. Reference is more suitable where only camera IDs are stored in each violation entry. Additional data such as speed limit can be looked up in the Camera collection. For consistency, maintaining references ensures up to date camera information for tracking without having to modify violation history.\n",
    "\n",
    "Conclusion:\n",
    "We choose to store references instead of embed data in order to reference read-heavy access to static data, write heavy streaming of violations, avoid duplicating daily violation with owner/camera information, prevent stale data and prevent risking document size limit as violation collection grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797ff17",
   "metadata": {},
   "source": [
    "### Task 1.3 Discussion\n",
    "\n",
    "In this task, discuss whether your model supports\n",
    "* Consistency and Idempotency\n",
    "    * Does it support idempotent writes?\n",
    "    * Explain any upsert pattern in `violation` collection\n",
    "* Scalability and Fault-Tolerance\n",
    "    * Can your data model support high ingest rates?\n",
    "    * Can your data model support low-latency lookups?\n",
    "\n",
    "Justify and explain the trade-off made in your design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8987c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eafe9a9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
    "\n",
    "This section consists of 2 sub-questions. \n",
    "\n",
    "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51a153",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"> \n",
    "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fc3a8",
   "metadata": {},
   "source": [
    "### Task 2.1 Data Stream Processing\n",
    "\n",
    "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
    "\n",
    "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989065f",
   "metadata": {},
   "source": [
    "#### Event Producer\n",
    "\n",
    "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7a85d",
   "metadata": {},
   "source": [
    "#### Streaming Application\n",
    "\n",
    "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
    "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
    "* If there is a violation detected, store it into MongoDB straight away.\n",
    "* If there is no violation detected, drop the record.\n",
    "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
    "\n",
    "##### Violation Detection Rule\n",
    "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
    "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
    "* Average speed of vehicle exceed the speed limit of the ending camera\n",
    "\n",
    "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308870bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MongoDB collection creation and data loading script\n",
    "\n",
    "# school\n",
    "# hostip = \"10.192.40.238\"\n",
    "# home\n",
    "# hostip = \"192.168.0.88\"\n",
    "# hostip = \"192.168.0.160\"\n",
    "# hostip = \"192.168.0.21\"\n",
    "\n",
    "hostip = \"10.192.41.222\"\n",
    "\n",
    "\n",
    "# 1. Connect to mongoDB server \n",
    "# MONGO_URI   = \"mongodb://{hostip}:27017\"\n",
    "# DB_NAME     = \"awas_db\"\n",
    "# VEH_CSV     = \"FIT3182-A2/data/vehicle.csv\"\n",
    "# CAM_CSV     = \"FIT3182-A2/data/camera.csv\"\n",
    "# HISTORIC_CSV = \"FIT3182-A2/data/camera_event_historic.csv\"\n",
    "# CHECKPOINT_PATH = \"/tmp/spark_checkpoints/awas_stream\"\n",
    "\n",
    "# MONGO_URI   = \"mongodb://{hostip}:27017\"\n",
    "# DB_NAME     = \"awas_db\"\n",
    "# VEH_CSV     = \"A2collab/FIT3182-A2/data/vehicle.csv\"\n",
    "# CAM_CSV     = \"A2collab/FIT3182-A2/data/camera.csv\"\n",
    "# HISTORIC_CSV = \"A2collab/FIT3182-A2/data/camera_event_historic.csv\"\n",
    "# CHECKPOINT_PATH = \"/tmp/spark_checkpoints/awas_stream\"\n",
    "\n",
    "#yl laptop\n",
    "MONGO_URI   = \"mongodb://{hostip}:27017\"\n",
    "DB_NAME     = \"awas_db\"\n",
    "VEH_CSV     = \"A2/data/vehicle.csv\"\n",
    "CAM_CSV     = \"A2/data/camera.csv\"\n",
    "HISTORIC_CSV = \"A2/data/camera_event_historic.csv\"\n",
    "CHECKPOINT_PATH = \"/tmp/spark_checkpoints/awas_stream\"\n",
    "\n",
    "def connect_client(hostip):\n",
    "    \"\"\"\n",
    "    Connect to MongoDB client with error handling.\n",
    "    \"\"\"\n",
    "    client = MongoClient(\n",
    "        host=f'{hostip}',\n",
    "        port=27017\n",
    "    )\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"✅ MongoDB connection successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ MongoDB connection failed: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    return client\n",
    "\n",
    "client = connect_client(hostip)\n",
    "\n",
    "# Connect to the database\n",
    "db = client[DB_NAME]\n",
    "\n",
    "# 2. Define JSON Schemas for Collections\n",
    "vehicle_schema = {\n",
    "    \"bsonType\": \"object\",\n",
    "    \"required\": [\"_id\",\"owner_name\",\"owner_addr\",\"vehicle_type\",\"registration_date\"],\n",
    "    \"properties\": {\n",
    "        \"_id\":               { \"bsonType\": \"string\" },\n",
    "        \"owner_name\":        { \"bsonType\": \"string\" },\n",
    "        \"owner_addr\":        { \"bsonType\": \"string\" },\n",
    "        \"vehicle_type\":      { \"bsonType\": \"string\" },\n",
    "        \"registration_date\": { \n",
    "            \"bsonType\": \"string\", \n",
    "            \"pattern\": r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z?$\"\n",
    "        }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "camera_schema = {\n",
    "    \"bsonType\": \"object\",\n",
    "    \"required\": [\"_id\",\"latitude\",\"longitude\",\"position_km\",\"speed_limit\"],\n",
    "    \"properties\": {\n",
    "        \"_id\":         { \"bsonType\": \"int\" },\n",
    "        \"latitude\":    { \"bsonType\": \"double\", \"minimum\": -90, \"maximum\": 90 },\n",
    "        \"longitude\":   { \"bsonType\": \"double\", \"minimum\": -180, \"maximum\": 180 },\n",
    "        \"position_km\": { \"bsonType\": \"double\" },\n",
    "        \"speed_limit\": { \"bsonType\": \"double\" }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "violation_schema = {\n",
    "    \"bsonType\": \"object\",\n",
    "    \"required\": [\"_id\", \"car_plate\", \"date\", \"violations\", \"total_violations\"],\n",
    "    \"properties\": {\n",
    "        \"_id\":            { \"bsonType\": \"string\" },\n",
    "        \"car_plate\":      { \"bsonType\": \"string\" },\n",
    "        \"date\":           { \"bsonType\": \"string\", \"pattern\": r\"^\\d{4}-\\d{2}-\\d{2}$\" },\n",
    "        \"violations\": {\n",
    "            \"bsonType\": \"array\",\n",
    "            \"items\": {\n",
    "                \"bsonType\": \"object\",\n",
    "                \"required\": [\n",
    "                    \"violation_id\",\"type\",\"camera_id_start\",\"camera_id_end\",\n",
    "                    \"timestamp_start\",\"recorded_speed\",\"speed_limit\"\n",
    "                ],\n",
    "                \"properties\": {\n",
    "                    \"violation_id\":     { \"bsonType\": \"string\" },\n",
    "                    \"type\":             { \"enum\": [\"instant\",\"average\"], \"bsonType\": \"string\" },\n",
    "                    \"camera_id_start\":  { \"bsonType\": \"int\" },\n",
    "                    \"camera_id_end\":    { \"bsonType\": \"int\" },\n",
    "                    \"timestamp_start\":  { \"bsonType\": \"string\"},\n",
    "                    \"timestamp_end\":    { \"anyOf\":[{\"bsonType\":\"string\"},{\"bsonType\":\"null\"}] },\n",
    "                    \"recorded_speed\":   { \"bsonType\": \"double\" },\n",
    "                    \"speed_limit\":      { \"bsonType\": \"double\" }\n",
    "                },\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        },\n",
    "        \"total_violations\": { \"bsonType\": \"int\", \"minimum\": 0 }\n",
    "    },\n",
    "    \"additionalProperties\": False\n",
    "}\n",
    " \n",
    "# 3. Recreate Collections with JSON Schema Validation\n",
    "def reset_collection(name, schema):\n",
    "    if name in db.list_collection_names():\n",
    "        db.drop_collection(name)\n",
    "    db.create_collection(name, validator={\"$jsonSchema\": schema})\n",
    "\n",
    "reset_collection(\"vehicle\", vehicle_schema)\n",
    "reset_collection(\"camera\", camera_schema)\n",
    "reset_collection(\"violation\", violation_schema)\n",
    "\n",
    "\n",
    "def create_indexes(db):\n",
    "    # vehicle: _id is primary key, also hint for hashed shard key\n",
    "    db.vehicle.create_index([(\"_id\", HASHED)])\n",
    "    # violation: _id is primary key, also hint for hashed shard key\n",
    "    db.violation.create_index([(\"car_plate\", HASHED)])\n",
    "    db.violation.create_index([(\"date\", ASCENDING)])\n",
    "\n",
    "create_indexes(db)\n",
    "\n",
    "# helper function to parse registration date\n",
    "def parse_reg_date(s: str) -> datetime:\n",
    "    return datetime.fromisoformat(s.rstrip(\"Z\"))\n",
    "\n",
    "# 4a. vehicle.csv\n",
    "def parse_vehicle_csv(file_path, database):\n",
    "    # Ensure the CSV file exists\n",
    "    if not file_path:\n",
    "        print(f\"CSV file {file_path} does not exist.\")\n",
    "        return\n",
    "    \n",
    "    with open(file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        latest_by_plate = {}  # car_plate -> (datetime, doc)\n",
    "\n",
    "        docs = []\n",
    "        for r in reader:\n",
    "            plate = r[\"car_plate\"]\n",
    "            reg_date_str = r[\"registration_date\"]\n",
    "            reg_dt = parse_reg_date(reg_date_str)\n",
    "\n",
    "            doc ={\n",
    "                \"_id\":               plate,\n",
    "                \"owner_name\":        r[\"owner_name\"],\n",
    "                \"owner_addr\":        r[\"owner_addr\"],\n",
    "                \"vehicle_type\":      r[\"vehicle_type\"],\n",
    "                \"registration_date\": reg_date_str\n",
    "            }\n",
    "\n",
    "            # only keep the latest registration date for each plate\n",
    "            if plate not in latest_by_plate or reg_dt > latest_by_plate[plate][0]:\n",
    "                latest_by_plate[plate] = (reg_dt, doc)\n",
    "\n",
    "        docs = [doc for (_dt, doc) in latest_by_plate.values()]\n",
    "        \n",
    "        if docs:\n",
    "            try:\n",
    "                database.vehicle.insert_many(docs, ordered=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting vehicle docs: {e}\")\n",
    "            print(f\"Inserted {len(docs)} vehicles\")\n",
    "\n",
    "# 4b. camera.csv\n",
    "def parse_camera_csv(file_path, database):\n",
    "    with open(file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        docs = []\n",
    "        for r in reader:\n",
    "            docs.append({\n",
    "                \"_id\":         int(r[\"camera_id\"]),\n",
    "                \"latitude\":    float(r[\"latitude\"]),\n",
    "                \"longitude\":   float(r[\"longitude\"]),\n",
    "                \"position_km\": float(r[\"position\"]),\n",
    "                \"speed_limit\": float(r[\"speed_limit\"])\n",
    "            })\n",
    "        if docs:\n",
    "            try:\n",
    "                database.camera.insert_many(docs, ordered=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Error inserting camera docs: {e}\")\n",
    "            print(f\"Inserted {len(docs)} camera docs\")\n",
    "\n",
    "\n",
    "# 4c. camera_event_history.csv\n",
    "def parse_historic_violations(file_path, database):\n",
    "    groups = {}  # key = (date, plate) → list of violation entries\n",
    "    with open(file_path, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            # parse fields\n",
    "            vid   = r[\"violation_id\"]\n",
    "            plate = r[\"car_plate\"]\n",
    "            start = r[\"timestamp_start\"]\n",
    "            end   = r[\"timestamp_end\"] or None\n",
    "            speed = float(r[\"speed_reading\"])\n",
    "            cam_s = int(r[\"camera_id_start\"])\n",
    "            cam_e = int(r[\"camera_id_end\"])\n",
    "\n",
    "            # derive date (YYYY-MM-DD)\n",
    "            date = start.split(\"T\",1)[0]\n",
    "\n",
    "            # determine type\n",
    "            vtype = \"instant\" if end is None else \"average\"\n",
    "\n",
    "            entry = {\n",
    "                \"violation_id\":    vid,\n",
    "                \"type\":            vtype,\n",
    "                \"camera_id_start\": cam_s,\n",
    "                \"camera_id_end\":   cam_e,\n",
    "                \"timestamp_start\": start,\n",
    "                \"timestamp_end\":   end,\n",
    "                \"recorded_speed\":  speed,\n",
    "                \"speed_limit\":     speed  # assume speed limit is same as recorded speed for simplicity\n",
    "            }\n",
    "\n",
    "            key = (date, plate)\n",
    "            groups.setdefault(key, []).append(entry)\n",
    "\n",
    "    docs = []\n",
    "    for (date, plate), violations in groups.items():\n",
    "        docs.append({\n",
    "            \"_id\":              f\"{date}_{plate}\",\n",
    "            \"car_plate\":        plate,\n",
    "            \"date\":             date,\n",
    "            \"violations\":       violations,\n",
    "            \"total_violations\": len(violations)\n",
    "        })\n",
    "\n",
    "    if docs:\n",
    "        try:\n",
    "            database.violation.insert_many(docs, ordered=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting violation docs: {e}\")\n",
    "        print(f\"Inserted {len(docs)} historic violations\")\n",
    "\n",
    "# 5. Parse and load CSV data\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"✅ MongoDB connection successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ MongoDB connection failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "try: \n",
    "    parse_vehicle_csv(VEH_CSV, db)\n",
    "    parse_camera_csv(CAM_CSV, db)\n",
    "    parse_historic_violations(HISTORIC_CSV, db)\n",
    "    print(\"✅ All collections created and CSV data loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error parsing CSV files: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages \"\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\"\n",
    "    \"org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,\"\n",
    "    \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 \"\n",
    "    \"pyspark-shell\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AWAS-Speed-Enforcement\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", f\"mongodb://{hostip}:27017/{DB_NAME}\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", f\"mongodb://{hostip}:27017/{DB_NAME}\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Define schema of incoming JSON messages\n",
    "eventSchema = (StructType()\n",
    "    .add(\"event_id\", StringType())\n",
    "    .add(\"car_plate\", StringType()) \n",
    "    .add(\"camera_id\", IntegerType()) \n",
    "    .add(\"timestamp\", TimestampType()) \n",
    "    .add(\"speed_reading\", DoubleType()) \n",
    "    .add(\"producer\", StringType()))\n",
    "\n",
    "rawA = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "    .option(\"subscribe\", \"ProducerA\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "producerA_sdf = rawA.select(from_json(col(\"value\").cast(\"string\"), eventSchema).alias(\"event\")).selectExpr(\"event.event_id\",\"event.car_plate\", \"event.camera_id\", \"event.timestamp\", \"event.speed_reading\")\n",
    "\n",
    "rawB = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "    .option(\"subscribe\", \"ProducerB\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "producerB_sdf = rawB.select(from_json(col(\"value\").cast(\"string\"), eventSchema).alias(\"event\")).selectExpr(\"event.event_id\",\"event.car_plate\", \"event.camera_id\", \"event.timestamp\", \"event.speed_reading\")\n",
    "\n",
    "rawC = (spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\")\n",
    "    .option(\"subscribe\", \"ProducerC\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load())\n",
    "\n",
    "producerC_sdf = rawC.select(from_json(col(\"value\").cast(\"string\"), eventSchema).alias(\"event\")).selectExpr(\"event.event_id\",\"event.car_plate\", \"event.camera_id\", \"event.timestamp\", \"event.speed_reading\")\n",
    "\n",
    "producerA_sdf = producerA_sdf.withWatermark(\"timestamp\", \"2 minutes\")\n",
    "producerB_sdf = producerB_sdf.withWatermark(\"timestamp\", \"2 minutes\")\n",
    "producerC_sdf = producerC_sdf.withWatermark(\"timestamp\", \"2 minutes\")\n",
    "\n",
    "camera_info = (spark.read\n",
    "               .format(\"mongodb\")\n",
    "               .option(\"collection\", \"camera\")\n",
    "               .load()\n",
    "               .selectExpr(\"_id as camera_id\", \"position_km as position\", \"speed_limit\")\n",
    ")\n",
    "\n",
    "cam = camera_info.alias(\"cam\")\n",
    "\n",
    "a = producerA_sdf.alias(\"a\")\n",
    "b = producerB_sdf.alias(\"b\")\n",
    "c = producerC_sdf.alias(\"c\")\n",
    "\n",
    "# join A_B\n",
    "joinA_B = a.join(\n",
    "    b,\n",
    "    expr(\n",
    "        \"\"\"\n",
    "        a.car_plate = b.car_plate AND \n",
    "        b.timestamp > a.timestamp AND \n",
    "        b.timestamp <= a.timestamp + interval 3 minutes\n",
    "        \"\"\"\n",
    "        ),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "joinA_B = joinA_B.join(camera_info.alias(\"cam1\"), col(\"cam1.camera_id\") == col(\"a.camera_id\")) \\\n",
    "                 .join(camera_info.alias(\"cam2\"), col(\"cam2.camera_id\") == col(\"b.camera_id\"))\n",
    "\n",
    "# Compute time difference in hours and distance in km\n",
    "joinA_B = joinA_B.withColumn(\"time_hours\", \n",
    "                              (unix_timestamp(col(\"b.timestamp\")) - unix_timestamp(col(\"a.timestamp\"))) / 3600.0) \\\n",
    "                 .withColumn(\"distance_km\", col(\"cam2.position\") - col(\"cam1.position\")) \\\n",
    "                 .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_hours\"))\n",
    "\n",
    "validAB = joinA_B.filter(\n",
    "    (col(\"b.timestamp\") > col(\"a.timestamp\")) &\n",
    "    (col(\"cam2.position\") > col(\"cam1.position\"))\n",
    ")\n",
    "\n",
    "# Identify violations between A and B\n",
    "violationsAB = joinA_B.filter(col(\"avg_speed\") > col(\"cam1.speed_limit\")) \\\n",
    "    .selectExpr(\"a.car_plate as car_plate\",\n",
    "                \"a.timestamp as timestamp_start\",\n",
    "                \"b.timestamp as timestamp_end\",\n",
    "                \"a.camera_id as camera_id_start\",\n",
    "                \"b.camera_id as camera_id_end\",\n",
    "                \"avg_speed as recorded_speed\",\n",
    "                \"cam2.speed_limit as speed_limit\",\n",
    "                \"cast('average' as string) as violation_type\")\n",
    "\n",
    "# Identify non-violations between A and B\n",
    "non_violationsAB = validAB.filter(col(\"avg_speed\") <= col(\"cam1.speed_limit\")) \\\n",
    "    .selectExpr(\"a.car_plate as car_plate\",\n",
    "                \"a.timestamp as dropped_at\",\n",
    "                \"cast('no_violation' as string) as drop_reason\")\n",
    "\n",
    "# left outer join A with B to find unmatched records\n",
    "joinA_B_left = a.join(\n",
    "    b,\n",
    "    expr(\n",
    "        \"\"\"\n",
    "        a.car_plate = b.car_plate AND \n",
    "        b.timestamp > a.timestamp AND \n",
    "        b.timestamp <= a.timestamp + interval 2 minutes\n",
    "        \"\"\"\n",
    "        ),\n",
    "    how=\"left_outer\"\n",
    ")\n",
    "\n",
    "# Filter out records where there is no match in B\n",
    "no_matchAB = joinA_B_left.filter(col(\"b.car_plate\").isNull()) \\\n",
    "    .selectExpr(\"a.car_plate as car_plate\",\n",
    "                \"a.timestamp as dropped_at\",\n",
    "                \"cast('no_match' as string) as drop_reason\")\n",
    "\n",
    "# join B_C\n",
    "joinB_C = b.join(\n",
    "    c,\n",
    "    expr(\n",
    "        \"\"\"\n",
    "        b.car_plate = c.car_plate AND \n",
    "        c.timestamp > b.timestamp AND \n",
    "        c.timestamp <= b.timestamp + interval 2 minutes\n",
    "        \"\"\"\n",
    "        ),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "joinB_C = joinB_C.join(camera_info.alias(\"cam2\"), col(\"cam2.camera_id\") == col(\"b.camera_id\")) \\\n",
    "                .join(camera_info.alias(\"cam3\"), col(\"cam3.camera_id\") == col(\"c.camera_id\"))\n",
    "\n",
    "# time difference \n",
    "joinB_C = joinB_C.withColumn(\"time_hours\", \n",
    "                              (unix_timestamp(col(\"c.timestamp\")) - unix_timestamp(col(\"b.timestamp\"))) / 3600.0) \\\n",
    "                 .withColumn(\"distance_km\", col(\"cam3.position\") - col(\"cam2.position\")) \\\n",
    "                 .withColumn(\"avg_speed\", col(\"distance_km\") / col(\"time_hours\"))\n",
    "\n",
    "validBC = joinB_C.filter(\n",
    "    (col(\"c.timestamp\") > col(\"b.timestamp\")) &\n",
    "    (col(\"cam3.position\") > col(\"cam2.position\"))\n",
    ")\n",
    "\n",
    "# identify violations\n",
    "violationsBC = joinB_C.filter(col(\"avg_speed\") > col(\"cam2.speed_limit\")) \\\n",
    "    .selectExpr(\"b.car_plate as car_plate\",\n",
    "                \"b.timestamp as timestamp_start\",\n",
    "                \"c.timestamp as timestamp_end\",\n",
    "                \"b.camera_id as camera_id_start\",\n",
    "                \"c.camera_id as camera_id_end\",\n",
    "                \"avg_speed as recorded_speed\",\n",
    "                \"cam2.speed_limit as speed_limit\",\n",
    "                \"cast('average' as string) as violation_type\")\n",
    "\n",
    "# identify non-violations\n",
    "non_violationsBC = validBC.filter(col(\"avg_speed\") <= col(\"cam2.speed_limit\")) \\\n",
    "    .selectExpr(\"b.car_plate as car_plate\",\n",
    "                \"c.timestamp as dropped_at\",\n",
    "                # \"avg_speed as recorded_speed\",      \n",
    "                # \"cam2.speed_limit as speed_limit\",\n",
    "                \"cast('no_violation' as string) as drop_reason\")\n",
    "\n",
    "# left outer join B with C to find unmatched records\n",
    "joinB_C_left = b.join(\n",
    "    c,\n",
    "    expr(\n",
    "        \"\"\"\n",
    "        b.car_plate = c.car_plate AND\n",
    "        c.timestamp > b.timestamp AND\n",
    "        c.timestamp <= b.timestamp + interval 2 minutes\n",
    "        \"\"\"\n",
    "        ),\n",
    "    how=\"left_outer\")\n",
    "\n",
    "# Filter out records where there is no match in C\n",
    "no_matchBC = joinB_C_left.filter(col(\"c.car_plate\").isNull()) \\\n",
    "    .selectExpr(\"b.car_plate as car_plate\",\n",
    "                \"b.timestamp as dropped_at\",\n",
    "                \"cast('no_match' as string) as drop_reason\")\n",
    "\n",
    "# identify instant violations\n",
    "instant_violA = a.join(cam, on=\"camera_id\") \\\n",
    "    .filter(col(\"speed_reading\") > col(\"speed_limit\")) \\\n",
    "    .selectExpr(\"car_plate\", \n",
    "                \"timestamp as timestamp_start\", \n",
    "                \"timestamp as timestamp_end\",\n",
    "                \"camera_id as camera_id_start\",\n",
    "                \"camera_id as camera_id_end\",\n",
    "                \"speed_reading as recorded_speed\",\n",
    "                \"speed_limit\", \n",
    "                \"cast('instant' as string) as violation_type\")\n",
    "\n",
    "instant_violB = b.join(cam, on=\"camera_id\") \\\n",
    "    .filter(col(\"speed_reading\") > col(\"speed_limit\")) \\\n",
    "    .selectExpr(\"car_plate\", \n",
    "                \"timestamp as timestamp_start\", \n",
    "                \"timestamp as timestamp_end\",\n",
    "                \"camera_id as camera_id_start\",\n",
    "                \"camera_id as camera_id_end\",\n",
    "                \"speed_reading as recorded_speed\",\n",
    "                \"speed_limit\", \n",
    "                \"cast('instant' as string) as violation_type\")\n",
    "\n",
    "instant_violC = c.join(cam, on=\"camera_id\") \\\n",
    "    .filter(col(\"speed_reading\") > col(\"speed_limit\")) \\\n",
    "    .selectExpr(\"car_plate\", \n",
    "                \"timestamp as timestamp_start\", \n",
    "                \"timestamp as timestamp_end\",\n",
    "                \"camera_id as camera_id_start\",\n",
    "                \"camera_id as camera_id_end\",\n",
    "                \"speed_reading as recorded_speed\",\n",
    "                \"speed_limit\", \n",
    "                \"cast('instant' as string) as violation_type\")\n",
    "\n",
    "# union all non-violations\n",
    "all_non_violations = non_violationsAB.union(non_violationsBC).union(no_matchAB).union(no_matchBC)\n",
    "# union all violations\n",
    "all_violations = violationsAB.union(violationsBC).union(instant_violA).union(instant_violB).union(instant_violC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_schema = StructType([\n",
    "#     StructField(\"car_plate\", StringType()),\n",
    "#     StructField(\"camera_id\", IntegerType()),\n",
    "#     StructField(\"timestamp\", TimestampType()),\n",
    "#     StructField(\"status\", StringType())\n",
    "# ])\n",
    "\n",
    "# # Define state schema\n",
    "# state_schema = StructType([\n",
    "#     StructField(\"camera_id\", IntegerType()),\n",
    "#     StructField(\"timestamp\", TimestampType())\n",
    "# ])\n",
    "\n",
    "# # Define the logic in Pandas\n",
    "# def match_and_timeout(batch: pd.DataFrame, state: pd.Series) -> pd.DataFrame:\n",
    "#     batch = batch.sort_values(\"timestamp\")\n",
    "#     output = []\n",
    "\n",
    "#     for _, row in batch.iterrows():\n",
    "#         ts = row[\"timestamp\"]\n",
    "#         cam = row[\"camera_id\"]\n",
    "\n",
    "#         if state and pd.notna(state.get(\"timestamp\")):\n",
    "#             prev_ts = pd.to_datetime(state[\"timestamp\"])\n",
    "#             prev_cam = state[\"camera_id\"]\n",
    "\n",
    "#             if ts > prev_ts and cam != prev_cam:\n",
    "#                 output.append({\n",
    "#                     \"car_plate\": row[\"car_plate\"],\n",
    "#                     \"camera_id\": cam,\n",
    "#                     \"timestamp\": ts,\n",
    "#                     \"status\": \"matched\"\n",
    "#                 })\n",
    "#                 state.remove()\n",
    "#             else:\n",
    "#                 state.update({\"timestamp\": ts, \"camera_id\": cam})\n",
    "#                 state.setTimeoutTimestamp(ts.timestamp() * 1000 + 3 * 60 * 1000)\n",
    "#         else:\n",
    "#             state.update({\"timestamp\": ts, \"camera_id\": cam})\n",
    "#             state.setTimeoutTimestamp(ts.timestamp() * 1000 + 3 * 60 * 1000)\n",
    "\n",
    "#     if state.hasTimedOut and state.exists:\n",
    "#         state_data = state.get\n",
    "#         output.append({\n",
    "#             \"car_plate\": batch.iloc[-1][\"car_plate\"],\n",
    "#             \"camera_id\": state_data[\"camera_id\"],\n",
    "#             \"timestamp\": state_data[\"timestamp\"],\n",
    "#             \"status\": \"unmatched\"\n",
    "#         })\n",
    "#         state.remove()\n",
    "\n",
    "#     return pd.DataFrame(output, columns=[\"car_plate\", \"camera_id\", \"timestamp\", \"status\"])\n",
    "\n",
    "\n",
    "# # Apply in Spark\n",
    "# matched_and_dropped = drop_monitor_df.groupBy(\"car_plate\").applyInPandasWithState(\n",
    "#     func=match_and_timeout,\n",
    "#     outputStructType=output_schema,\n",
    "#     stateStructType=state_schema,\n",
    "#     outputMode=\"append\",\n",
    "#     timeoutConf=GroupStateTimeout.EventTimeTimeout\n",
    "# )\n",
    "\n",
    "violation_collection = db.violation\n",
    "\n",
    "# write violations to MongoDB\n",
    "\n",
    "def write_to_mongo(batch_df, batch_id):\n",
    "    violations = batch_df.collect()\n",
    "    for row in violations:\n",
    "\n",
    "        # create violation document\n",
    "        plate = row['car_plate']\n",
    "        date_str = row['timestamp_start'].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        vid = str(uuid.uuid4())\n",
    "\n",
    "        violation = {\n",
    "            \"violation_id\":    vid,\n",
    "            \"type\": row['violation_type'],\n",
    "            \"camera_id_start\": row['camera_id_start'],\n",
    "            \"camera_id_end\": row['camera_id_end'],\n",
    "            \"timestamp_start\": row['timestamp_start'].isoformat(),\n",
    "            \"timestamp_end\": row['timestamp_end'].isoformat() if row['timestamp_end'] else None,\n",
    "            \"recorded_speed\": row['recorded_speed'],\n",
    "            \"speed_limit\": row['speed_limit']\n",
    "        }\n",
    "\n",
    "        doc_id = f\"{date_str}_{plate}\"\n",
    "        filter_doc = { \"_id\": doc_id }\n",
    "\n",
    "        update_doc = {\n",
    "            \"$setOnInsert\": { \"_id\": doc_id, \"car_plate\": plate, \"date\": date_str}, \n",
    "            \"$addToSet\": {\"violations\": violation},\n",
    "            \"$inc\": {\"total_violations\": 1}\n",
    "        }\n",
    "\n",
    "        # upsert into MongoDB\n",
    "        if violation_collection.count_documents(filter_doc) > 0:\n",
    "            # if document exists, update it\n",
    "            violation_collection.update_one(filter_doc, update_doc)\n",
    "        else:\n",
    "            violation_collection.insert_one({\n",
    "                \"_id\": doc_id,\n",
    "                \"car_plate\": plate,\n",
    "                \"date\": date_str,\n",
    "                \"violations\": [violation],\n",
    "                \"total_violations\": 1\n",
    "            })\n",
    "            \n",
    "        # violation_collection.update_one(filter_doc, update_doc, upsert=True)\n",
    "\n",
    "    print(f\"Batch {batch_id} processed: {len(violations)} violations written to MongoDB, total {violation_collection.count_documents({})} violations in collection.\")\n",
    "\n",
    "\n",
    "# Start the streaming queries\n",
    "dropped_query = (all_non_violations.writeStream\n",
    "        .format(\"console\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"truncate\", False)\n",
    "        .start())\n",
    "\n",
    "query = (all_violations.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "         .foreachBatch(write_to_mongo)\n",
    "         .start()\n",
    "         )\n",
    "\n",
    "query.awaitTermination()\n",
    "dropped_query.awaitTermination()\n",
    "# Stop the Spark session when done\n",
    "spark.stop()\n",
    "# Close the MongoDB connection\n",
    "client.close()\n",
    "print(\"Stream processing complete. MongoDB connection closed.\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598abe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# CHECKPOINT_PATH = \"/tmp/spark_checkpoints/awas_stream\"\n",
    "\n",
    "# # === 1. Delete Spark checkpoint directory ===\n",
    "# if os.path.exists(CHECKPOINT_PATH):\n",
    "#     shutil.rmtree(CHECKPOINT_PATH)\n",
    "#     print(f\"✅ Deleted checkpoint directory: {CHECKPOINT_PATH}\")\n",
    "# else:\n",
    "#     print(\"⚠️ Checkpoint directory not found, skipping.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11c4a",
   "metadata": {},
   "source": [
    "### Task 2.2 Data Visualisation\n",
    "\n",
    "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s), \n",
    "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values. \n",
    "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
    "\n",
    "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
    "* number of violations between the checkpoints\n",
    "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
    "\n",
    "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
    "\n",
    "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b82f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
    "\n",
    "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29b622",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
    "\n",
    "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
    "\n",
    "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
    "\n",
    "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
